{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "localpath_prefix='/Users/haizhao/OneDrive - Columbia University Irving Medical Center'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total_PPIs from ZEPPI: 9863206\n",
      "CPU times: user 4.09 s, sys: 582 ms, total: 4.67 s\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title Load Human_ZEPPI data files; takes ~10s after files are ready to read\n",
    "\n",
    "fileread1=localpath_prefix + '/WORK/PrePPI_MI/Human_af/human_af_lrg28_ZIPPI_06132023.csv'\n",
    "fileread2=localpath_prefix + '/WORK/PrePPI_MI/Human_af/human_af_lrg4l28_ZEPPI_08222023.csv' \n",
    "fileread3=localpath_prefix + '/WORK/PrePPI_MI/Human_af/human_af.lrg1l4_ZEPPI_03142024.csv'\n",
    "fileread4=localpath_prefix + '/WORK/PrePPI_MI/Human_af/human_af.lrl1_ZEPPI_03142024.csv'\n",
    "\n",
    "df_g28 = pd.read_csv(fileread1,sep=',',skiprows=0,usecols=['Uni_pair','LR','Neff','Nifr','Zmaxmt'])\n",
    "df_g4l28 = pd.read_csv(fileread2,sep=',',skiprows=0,usecols=['Uni_pair','LR','Neff','Nifr','Zmaxmt'])\n",
    "df_g1l4 = pd.read_csv(fileread3,sep=',',skiprows=0,usecols=['Uni_pair','LR','Neff','Nifr','Zmaxmt'])\n",
    "df_l1 = pd.read_csv(fileread4,sep=',',skiprows=0,usecols=['Uni_pair','LR','Neff','Nifr','Zmaxmt'])\n",
    "\n",
    "df_lr_g0 = pd.concat([df_g28, df_g4l28, df_g1l4, df_l1], ignore_index=True)\n",
    "\n",
    "print(\"Number of total_PPIs from ZEPPI:\",len(df_lr_g0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59550146"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read DS predictions; takes <1min after files are ready. \n",
    "\n",
    "filetoread=localpath_prefix+'/WORK/PrePPI_MI/DS/human/human_part1_withhomo.tsv'\n",
    "df_DS=pd.read_csv(filetoread,sep='\\t',names=[\"Uniprot1\", \"Uniprot2\",'DS'])\n",
    "df_DS = df_DS.dropna()\n",
    "df_DS['Uni_pair']=df_DS['Uniprot1']+\"_\"+ df_DS['Uniprot2']\n",
    "len(df_DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of HINT_LC_humanPPI: 16167\n",
      "Number of allExpDB_PPIs: 766045\n",
      "Number of allExpPhy_PPIs: 1200001\n",
      "Number of STRING-Physical Human (exp >0):  272379\n"
     ]
    }
   ],
   "source": [
    "#--- read human HINT HQ-LC and allDBs; \n",
    "\n",
    "def readcsv_return_pair(filename,sep_symbol,skipN=0):\n",
    "    df0=pd.read_csv(filename,sep=sep_symbol,skiprows=skipN) #names=[\"hfpd_ID\", \"Uni\"]\n",
    "    ref_pairs=df0['UniprotID_A_UniprotID_B'].tolist()\n",
    "    return ref_pairs\n",
    "\n",
    "def readcsv2_return_sortedpair(filename,sep_symbol,skipN=0):\n",
    "    df0=pd.read_csv(filename,sep=sep_symbol,skiprows=skipN) #names=[\"hfpd_ID\", \"Uni\"]\n",
    "    ref_pairs=tuple(zip(df0['UniprotID_A'], df0['UniprotID_B']))\n",
    "    ref_pairs_sorted=[]\n",
    "    for i in ref_pairs:\n",
    "        i = '_'.join(sorted(i))\n",
    "        if i not in ref_pairs_sorted:\n",
    "            ref_pairs_sorted.append(i)\n",
    "    return ref_pairs_sorted\n",
    "\n",
    "HINT_hq_bi_LC_pairs = readcsv2_return_sortedpair(localpath_prefix+'/WORK/Human_DB/Expts/2021HINT_Human_lcb_hq.txt','\\t')\n",
    "df_hqLC = pd.DataFrame(HINT_hq_bi_LC_pairs, columns=['Uni_pair'])\n",
    "print(\"Number of HINT_LC_humanPPI:\",len(df_hqLC))\n",
    "\n",
    "#--- Read allExpDBs database\n",
    "allExpPhy_file = localpath_prefix+'/WORK/Human_DB/human_ExpPhy_details.csv'\n",
    "allExpPhy_df = pd.read_csv(allExpPhy_file,skiprows=0,usecols=[0],names=[\"Uni_pair\"])\n",
    "print(\"Number of allExpDB_PPIs:\",len(allExpPhy_df))\n",
    "\n",
    "allExpDB_file = localpath_prefix+'/WORK/Human_DB/human_allExpDB.csv'\n",
    "allExpDB_df = pd.read_csv(allExpDB_file,skiprows=0,usecols=[0],names=[\"Uni_pair\"])\n",
    "print(\"Number of allExpPhy_PPIs:\",len(allExpDB_df))\n",
    "\n",
    "#--- Read STRING database\n",
    "def readcsv2_return_pair(filename,sep_symbol,skipN=0):\n",
    "    df0=pd.read_csv(filename,sep=sep_symbol,skiprows=skipN)\n",
    "    return df0['UniprotID_A_UniprotID_B'].tolist()\n",
    "\n",
    "toreadfile1 = localpath_prefix+'/WORK/Human_DB/Expts/2022String_9606.physical.links.experimental.sortedpair.nonred.csv'\n",
    "\n",
    "STRING_Human_exp_pairs = readcsv2_return_pair(toreadfile1,',')\n",
    "df_STRING = pd.DataFrame(STRING_Human_exp_pairs, columns=['Uni_pair'])\n",
    "print(\"Number of STRING-Physical Human (exp >0): \",len(STRING_Human_exp_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define function to assign labels; \n",
    "\n",
    "def AssignLabels(df,pos,allExpDB):\n",
    "    df_inPos = df[df['Uni_pair'].isin(pos)].copy()\n",
    "    print(\"Number of input_PPIs in Pos_set:\",len(df_inPos))\n",
    "\n",
    "    ratio=1000; n=ratio * len(df_inPos)\n",
    "    df_notinExpDB = df[~df['Uni_pair'].isin(allExpDB)].copy()\n",
    "    if n < len(df_notinExpDB):\n",
    "        df_notinExpDB_sampled = df_notinExpDB.sample(n)\n",
    "    else:\n",
    "        df_notinExpDB_sampled = df_notinExpDB\n",
    "\n",
    "    print(\"Number of input_PPIs in ExpDBs:\",len(df)-len(df_notinExpDB))\n",
    "    print(\"Number of input_PPIs not in ExpDBs:\",len(df_notinExpDB))\n",
    "\n",
    "    df_inPos['label'] = 1\n",
    "    df_notinExpDB_sampled['label'] = 0\n",
    "    df_inPosNeg = pd.concat([df_inPos,df_notinExpDB_sampled])\n",
    "    return df_inPosNeg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input_PPIs in Pos_set: 7582\n",
      "Number of input_PPIs in ExpDBs: 108287\n",
      "Number of input_PPIs not in ExpDBs: 9754919\n"
     ]
    }
   ],
   "source": [
    "#@title Assign labels; Prepare features for ZEPPI \n",
    "\n",
    "df_ZEPPI_inHINT_Unknown_totrain = AssignLabels(df_lr_g0,df_hqLC['Uni_pair'],allExpDB_df['Uni_pair'])\n",
    "df_ZEPPI_inHINT_Unknown_totrain = df_ZEPPI_inHINT_Unknown_totrain.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input_PPIs in Pos_set: 45465\n",
      "Number of input_PPIs in ExpDBs: 228852\n",
      "Number of input_PPIs not in ExpDBs: 59321294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Assign labels; Prepare features for DS (takes >10min due to large data size)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_DS_inSTRING_Unknown_totrain \u001b[38;5;241m=\u001b[39m \u001b[43mAssignLabels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_DS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_STRING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUni_pair\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mallExpDB_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUni_pair\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_DS_inSTRING_Unknown_totrain \u001b[38;5;241m=\u001b[39m df_DS_inSTRING_Unknown_totrain\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mAssignLabels\u001b[0;34m(df, pos, allExpDB)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mAssignLabels\u001b[39m(df,pos,allExpDB):\n\u001b[0;32m----> 4\u001b[0m     df_inPos \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUni_pair\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of input_PPIs in Pos_set:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(df_inPos))\n\u001b[1;32m      7\u001b[0m     ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m; n\u001b[38;5;241m=\u001b[39mratio \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_inPos)\n",
      "File \u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/pandas/core/series.py:5563\u001b[0m, in \u001b[0;36mSeries.isin\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   5490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misin\u001b[39m(\u001b[38;5;28mself\u001b[39m, values) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   5491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5492\u001b[0m \u001b[38;5;124;03m    Whether elements in Series are contained in `values`.\u001b[39;00m\n\u001b[1;32m   5493\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5561\u001b[0m \u001b[38;5;124;03m    dtype: bool\u001b[39;00m\n\u001b[1;32m   5562\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5563\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(result, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   5565\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5566\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/pandas/core/algorithms.py:523\u001b[0m, in \u001b[0;36misin\u001b[0;34m(comps, values)\u001b[0m\n\u001b[1;32m    520\u001b[0m     comps_array \u001b[38;5;241m=\u001b[39m comps_array\u001b[38;5;241m.\u001b[39mastype(common, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    521\u001b[0m     f \u001b[38;5;241m=\u001b[39m htable\u001b[38;5;241m.\u001b[39mismember\n\u001b[0;32m--> 523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomps_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Assign labels; Prepare features for DS (takes >10min due to large data size)\n",
    "\n",
    "df_DS_inSTRING_Unknown_totrain = AssignLabels(df_DS,df_STRING['Uni_pair'],allExpDB_df['Uni_pair'])\n",
    "df_DS_inSTRING_Unknown_totrain = df_DS_inSTRING_Unknown_totrain.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN values:\n",
      "Empty DataFrame\n",
      "Columns: [Uni_pair, Neff, Nifr, LR, Zmaxmt, label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#@title Check training data of ZEPPI and DS(TT) before starting training \n",
    "\n",
    "for i in [df_ZEPPI_inHINT_Unknown_totrain, df_DS_inSTRING_Unknown_totrain]:\n",
    "    data = i \n",
    "\n",
    "    # Check NaN values\n",
    "    nan_values = data.isna().any()\n",
    "\n",
    "    # Print columns with NaN values\n",
    "    rows_with_nan = data[data.isna().any(axis=1)]\n",
    "\n",
    "    print(\"Rows with NaN values:\")\n",
    "    print(rows_with_nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Discretized Naive Bayes (Naive, Simple and Independent)\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import importlib\n",
    "from src import Bayes\n",
    "importlib.reload(Bayes)\n",
    "\n",
    "from src.Bayes import DiscretizedNaiveBayes_IndependentFeatures\n",
    "from src.Bayes import DiscretizedNaiveBayes_JointBins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.56 s, sys: 1.49 s, total: 11.1 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title Split training and testing set; Train ZEPPI_LR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train_, X_test_, y_train_, y_test_ = train_test_split(data[['Zmaxmt']], data['label'], test_size=0.001, random_state=40)\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(data[['Zmaxmt','Neff','Nifr']], data['label'], test_size=0.001, random_state=40)\n",
    "\n",
    "#X_trainB3, X_testB3, y_trainB3, y_testB3 = train_test_split(data[['COV', 'SIZ',  'OL', 'OS']], data['label'], test_size=0.2, random_state=40)\n",
    "\n",
    "train_indices = np.sort(X_train_.index)\n",
    "train_UniPairs = data.loc[train_indices, \"Uni_pair\"]\n",
    "test_indices = np.sort(X_test_.index)\n",
    "\n",
    "X_test_sorted = X_test_.sort_index()\n",
    "y_test_sorted = y_test_.sort_index()\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "X_train0 = X_train_.to_numpy()\n",
    "y_train0 = y_train_.to_numpy()\n",
    "X_test0 = X_test_.to_numpy()\n",
    "y_test0 = y_test_.to_numpy()\n",
    "X_test_sorted = X_test_sorted.to_numpy()\n",
    "y_test_sorted = y_test_sorted.to_numpy()\n",
    "\n",
    "#nbn_ZP = DiscretizedNaiveBayes_IndependentFeatures()\n",
    "nbn_ZP =DiscretizedNaiveBayes_JointBins()\n",
    "\n",
    "nbn_ZP.fit(X_train0, y_train0)\n",
    "\n",
    "#data_test = data.loc[~data.index.isin(train_indices)]\n",
    "#print(len(X_test_sorted),len(data_test),data_test.index)\n",
    "\n",
    "#data_test['LR_ZP'] = nbn1.predict(X_test_sorted)\n",
    "#data_test['LRSM_LRZP'] = data_test['LR'] * data_test['LR_ZP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained ZEPPI_LR model using pickle\n",
    "import pickle\n",
    "with open('../data/human_ZP_BayesB3_ratio1000.pkl', 'wb') as f:\n",
    "    pickle.dump(nbn_ZP, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_classes [0 1]\n",
      "dict_keys([36, 19, 0, 3, 1, 20, 8, 9, 33, 5, 17, 38, 25, 23, 2, 11, 27, 6, 29, 13, 7, 22, 16, 37, 4, 31, 34, 18, 10, 24, 26, 12, 15, 35, 21, 14, 30, 28, 32]) dict_values([179533, 9827960, 24831769, 461698, 2269159, 74071, 147846, 219575, 405375, 2202831, 83992, 57758, 60335, 57533, 2037017, 105921, 61571, 216151, 108861, 93985, 190241, 58557, 79201, 99579, 277285, 76086, 70899, 168371, 119964, 58149, 67830, 117714, 81197, 86228, 62406, 85224, 67769, 64776, 85109])\n",
      "dict_keys([1, 0, 9, 35, 2, 33, 36, 37, 19, 23, 29, 3, 4, 5, 31, 12, 30, 11, 32, 10, 13, 28, 25, 24, 22, 34, 26, 16, 27, 17, 18, 6, 7, 20, 38, 14, 15, 8, 21]) dict_values([2897, 18698, 459, 1096, 1942, 925, 1642, 1792, 2320, 263, 468, 1053, 748, 1252, 483, 333, 437, 340, 564, 367, 315, 360, 288, 312, 259, 831, 298, 283, 347, 274, 276, 525, 537, 279, 922, 287, 272, 413, 271])\n",
      "CPU times: user 22.7 s, sys: 3min 15s, total: 3min 37s\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#@title Split training and testing set; Train DS(TT)_LR\n",
    "data2 = df_DS_inSTRING_Unknown_totrain\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(data2[['DS']], data2['label'], test_size=0.001, random_state=40)\n",
    "\n",
    "train_indices = np.sort(X_train_.index)\n",
    "train_UniPairs = data2.loc[train_indices, \"Uni_pair\"]\n",
    "test_indices = np.sort(X_test_.index)\n",
    "\n",
    "X_test_sorted = X_test_.sort_index()\n",
    "y_test_sorted = y_test_.sort_index()\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "X_train0 = X_train_.to_numpy()\n",
    "y_train0 = y_train_.to_numpy()\n",
    "X_test0 = X_test_.to_numpy()\n",
    "y_test0 = y_test_.to_numpy()\n",
    "X_test_sorted = X_test_sorted.to_numpy()\n",
    "y_test_sorted = y_test_sorted.to_numpy()\n",
    "\n",
    "nbn2 = DiscretizedNaiveBayes_IndependentFeatures()\n",
    "nbn2.fit(X_train0, y_train0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained DS(TT)_LR model using pickle\n",
    "\n",
    "with open('../data/human_DSwthomo_Bayes_STRING.pkl', 'wb') as f:\n",
    "    pickle.dump(nbn2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import ClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "\n",
    "data2_test = data2.loc[~data2.index.isin(train_indices)]\n",
    "data2_test['LR_DS'] = nbn2.predict(X_test_sorted)\n",
    "\n",
    "evaluator2 = ClassificationEvaluator('$LR^{DS}$ on HINT-LC', data2_test['label'].to_numpy(), data2_test['LR_DS'].to_numpy())\n",
    "evaluator2.plot_roc_curve_ratio()\n",
    "evaluator2.plot_prc_curve_ratio()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
